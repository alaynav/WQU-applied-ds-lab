# WQU Applied Data Science Lab
**Description:** Here you can find an overview of all eight projects that I completed during my enrollment at WorldQuant University (WQU), each designed to enhance my understanding of key data science concepts through data exploration, cleaning, analysis, and modeling. Using real-world datasets, I devoted one month per project and over 200 hours in Python/Jupyter Notebook.

Here are comprehensive details about the projects and the skills applied:

1) 𝗛𝗢𝗨𝗦𝗜𝗡𝗚 𝗜𝗡 𝗠𝗘𝗫𝗜𝗖𝗢: Learners use a dataset of 21,000 properties to determine if real estate prices are influenced more by property size or location. They import and clean data from a CSV file, build data visualizations, and examine the relationship between two variables using correlation.
    * Explored the impact of property size versus location on real estate prices
    * Utilized pandas DataFrame for data loading and cleaning
    * Created insightful visualizations to examine the relationship between variables

2) 𝗔𝗣𝗔𝗥𝗧𝗠𝗘𝗡𝗧 𝗦𝗔𝗟𝗘𝗦 𝗜𝗡 𝗕𝗨𝗘𝗡𝗢𝗦 𝗔𝗜𝗥𝗘𝗦: Learners build a linear regression model to predict apartment prices in Argentina. They create a data pipeline to impute missing values and encode categorical features, and they improve model performance by reducing overfitting.
    * Built a linear regression model to predict apartment prices in Argentina
    * Implemented a machine learning pipeline with feature encoding and imputation
    * Visualized model coefficients and insights using Mapbox scatter plots and heatmaps

3) 𝗔𝗜𝗥 𝗤𝗨𝗔𝗟𝗜𝗧𝗬 𝗜𝗡 𝗡𝗔𝗜𝗥𝗢𝗕𝗜: Learners build an ARMA time-series model to predict particulate matter levels in Kenya. They extract data from a MongoDB database using pymongo, and improve model performance through hyperparameter tuning.
    * Developed an ARMA time-series model to predict particulate matter levels in Kenya
    * Retrieved data from a MongoDB database and performed time series data exploration
    * Tuned hyperparameters for improved model performance

4) 𝗘𝗔𝗥𝗧𝗛𝗤𝗨𝗔𝗞𝗘 𝗗𝗔𝗠𝗔𝗚𝗘 𝗜𝗡 𝗡𝗘𝗣𝗔𝗟: Learners build logistic regression and decision tree models to predict earthquake damage to buildings. They extract data from a SQLite database, and reveal the biases in data that can lead to discrimination.
    * Constructed logistic regression and decision tree models to predict earthquake damage to buildings
    * Extracted data from a SQL database and performed model evaluation through train-test splits
    * Fine-tuned model hyperparameters for enhanced predictive accuracy

5) 𝗕𝗔𝗡𝗞𝗥𝗨𝗣𝗧𝗖𝗬 𝗜𝗡 𝗣𝗢𝗟𝗔𝗡𝗗: Learners build random forest and gradient boosting models to predict whether a company will go bankrupt. They navigate the Linux command line, address imbalanced data through resampling, and consider the impact of performance metrics precision and recall.
    * Built random forest and gradient boosting models to predict company bankruptcy
    * Handled imbalanced data using resampling techniques
    * Evaluated model performance using precision and recall metrics

6) 𝗖𝗨𝗦𝗧𝗢𝗠𝗘𝗥 𝗦𝗘𝗚𝗠𝗘𝗡𝗧𝗔𝗧𝗜𝗢𝗡 𝗜𝗡 𝗧𝗛𝗘 𝗨𝗦: Learners build a k-means model to cluster US consumers into groups. They use principal component analysis (PCA) for data visualization, and they create an interactive dashboard with Plotly Dash.
    * Employed k-means clustering to segment U.S. consumers into distinct groups
    * Conducted exploratory data analysis and feature selection for clustering
    * Deployed a Dash web application for interactive visualization

7) 𝗔/𝗕 𝗧𝗘𝗦𝗧𝗜𝗡𝗚 𝗔𝗧 𝗪𝗢𝗥𝗟𝗗𝗤𝗨𝗔𝗡𝗧 𝗨𝗡𝗜𝗩𝗘𝗥𝗦𝗜𝗧𝗬: Learners conduct a chi-square test to determine if sending an email can increase program enrollment at WQU. They build custom Python classes to implement an ETL process, and they create an interactive data application following a three-tiered design pattern.
    * Conducted chi-square tests to assess the impact of email campaigns on program enrollment
    * Designed experiments and analyze results using statistical methods
    * Built an interactive web application for data visualization and analysis

8) 𝗩𝗢𝗟𝗔𝗧𝗜𝗟𝗜𝗧𝗬 𝗙𝗢𝗥𝗘𝗖𝗔𝗦𝗧𝗜𝗡𝗚 𝗜𝗡 𝗜𝗡𝗗𝗜𝗔: Learners create a GARCH time series model to predict asset volatility. They acquire stock data through an API, clean and store it in a SQLite database, and build their own API to serve model predictions.
    * Created a GARCH time series model to predict asset volatility
    * Retrieve data from web APIs and load it into a SQL database.
    * Build a custom web API to serve model predictions

Thank you to WorldQuant and Professor Nicholas Cifuentes-Goodbody for providing such an incredible learning experience. I strongly recommend this curriculum to everyone fascinated by the exciting realm of Data Science!

---
**NOTE:** *The code content of each project cannot be uploaded due to copyright issues.*
